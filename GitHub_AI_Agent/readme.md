# ğŸš€ AI-Powered Data Cleaning Pipeline

## ğŸ“Œ Overview  
This project is a modular **AI-driven data processing pipeline** designed to **clean, validate, and analyze datasets**. It enables users to:  
âœ” Upload local datasets  
âœ” Scrape tables from websites  
âœ” Download and merge Kaggle datasets  
âœ” Perform automated **data cleaning** using AI and ML models  
âœ” Validate data transformations with **AI-generated insights**  

The system is built with **modularity and scalability** in mind, allowing future extensions for more advanced analytics.

---

## âš™ï¸ How It Works  
The project is structured into **three main phases**:

### **1ï¸âƒ£ Data Ingestion**
- Users can **upload a file**, **scrape data from the web**, or **retrieve a dataset from Kaggle**.
- The system extracts, processes, and standardizes the data.

### **2ï¸âƒ£ Data Cleaning (AI-Driven)**
- **Missing values** are handled using **KNN Imputation**.
- **Outliers** are detected and removed using a **CatBoost classifier**.
- **Column data types** are inferred and enforced using **OpenAIâ€™s GPT model**.

### **3ï¸âƒ£ Data Validation & Export**
- The cleaned dataset is **validated** by an AI-powered **Supervisor Agent**.
- Users receive **a report on data quality, transformations, and potential improvements**.
- The cleaned data is saved in the `cleaned_data/` folder for further analysis.

---

## ğŸ—ï¸ Project Structure  

```plaintext
ğŸ“‚ AI-Data-Cleaning-Pipeline
â”‚â”€â”€ ğŸ“‚ data_cleaning
â”‚   â”‚â”€â”€ cleaning_agent.py      # AI-based data cleaning module
â”‚   â”‚â”€â”€ config.py              # Loads API keys & configurations
â”‚   â”‚â”€â”€ outlier_model.pkl      # Pre-trained CatBoost outlier model
â”‚
â”‚â”€â”€ ğŸ“‚ data_scraper
â”‚   â”‚â”€â”€ scraper.py             # Web scraping module (table extraction)
â”‚
â”‚â”€â”€ ğŸ“‚ user_interface
â”‚   â”‚â”€â”€ uploader.py            # Handles dataset uploads & merging
â”‚
â”‚â”€â”€ ğŸ“‚ tests
â”‚   â”‚â”€â”€ test_cleaning.py       # Unit tests for data cleaning functions
â”‚   â”‚â”€â”€ test_scraper.py        # Unit tests for web scraping functions
â”‚
â”‚â”€â”€ ğŸ“‚ cleaned_data             # Stores all cleaned datasets
â”‚â”€â”€ main.py                    # Entry point for the pipeline
â”‚â”€â”€ gpt_api_key.txt             # OpenAI API key (DO NOT COMMIT TO GITHUB)
â”‚â”€â”€ requirements.txt            # Python dependencies
â”‚â”€â”€ README.md                   # Project documentation
```

---

## ğŸ”§ How to Use

### **1ï¸âƒ£ Running the Pipeline**
1. Clone this repository and navigate to the project folder.
   ```bash
   git clone https://github.com/your-repo/AI-Data-Cleaning-Pipeline.git
   cd AI-Data-Cleaning-Pipeline
   ```
2. Install required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Run the main script:
   ```bash
   python main.py
   ```

### **2ï¸âƒ£ Choosing a Data Source**
Upon running the script, users will be prompted to:
- Upload a local dataset (.csv or .xlsx) via a **file selection window**.
- Enter a URL to **scrape tabular data from a website**.
- Provide a **Kaggle dataset URL** for automated download & merging.

### **3ï¸âƒ£ Data Cleaning Process**
Once the dataset is loaded, the AI cleaning agent:
- Handles missing values, outliers, and incorrect data types.
- Uses OpenAI's GPT model for column type inference.
- Stores the cleaned dataset in **cleaned_data/**.

### **4ï¸âƒ£ Viewing Cleaning Reports**
The **Supervisor AI Agent** generates a detailed validation report on:
- Changes made to the dataset.
- Remaining issues and suggested improvements.

---

## ğŸ“‚ Data Storage & Output
- **Original datasets remain unchanged**.
- **Cleaned datasets** are stored in the `cleaned_data/` folder.
- **Kaggle datasets** are stored in `kaggle_data/`.
- **Scraped datasets** are processed dynamically.

---

## ğŸ”‘ API & Configuration
- Ensure `gpt_api_key.txt` is present with a **valid OpenAI API key**.
- The **pre-trained CatBoost outlier detection model** is stored in `outlier_model.pkl`.
- Modify hyperparameters (e.g., KNN neighbors, CatBoost depth) as needed.

---

## ğŸ› ï¸ Dependencies
Install required Python libraries with:
```bash
pip install -r requirements.txt
```

---

## ğŸ“¬ Contact & Contributions
For issues, feature requests, or contributions, open a GitHub issue or submit a pull request. Let's build a more powerful AI-driven data cleaning tool together! ğŸš€

