# ğŸš€ AI-Powered Data Cleaning Pipeline

## ğŸ“Œ Table of Contents
- [Overview](#overview)
- [How It Works](#how-it-works)
  - [1ï¸âƒ£ Data Ingestion](#1ï¸âƒ£-data-ingestion)
  - [2ï¸âƒ£ Data Cleaning (AI-Driven)](#2ï¸âƒ£-data-cleaning-ai-driven)
  - [3ï¸âƒ£ Data Validation & Export](#3ï¸âƒ£-data-validation--export)
- [Project Structure](#ğŸ—ï¸-project-structure)
- [How to Use](#ğŸ”§-how-to-use)
  - [Running the Pipeline](#1ï¸âƒ£-running-the-pipeline)
  - [Choosing a Data Source](#2ï¸âƒ£-choosing-a-data-source)
  - [Data Cleaning Process](#3ï¸âƒ£-data-cleaning-process)
  - [Viewing Cleaning Reports](#4ï¸âƒ£-viewing-cleaning-reports)
- [Data Storage & Output](#ğŸ“‚-data-storage--output)
- [AI Agents & Customization](#ğŸ”‘-ai-agents--customization)
- [Dependencies](#ğŸ› ï¸-dependencies)
- [Contact & Contributions](#ğŸ“¬-contact--contributions)
- [Why Use AI Agents for Data Cleaning?](#ğŸ”¥-why-use-ai-agents-for-data-cleaning)
- [Get Started](#ğŸš€-ready-to-automate-your-data-cleaning)

---

## ğŸ“Œ Overview  
This project is a modular **AI-driven data processing pipeline** designed to **clean, validate, and analyze datasets** using intelligent **AI Agents**. These AI Agents automate data transformations and validation, ensuring high data quality.  

### ğŸ”¥ **What Makes This AI-Powered?**
- Uses **OpenAIâ€™s GPT models** to **understand and validate column types**.  
- Uses **KNN Imputation & Mode Filling** for intelligent **missing value handling**.  
- Uses a **pre-trained CatBoost model** for **outlier detection & removal**.  
- AI **Supervisor Agent** provides **post-cleaning validation & suggestions**.  

---

## âš™ï¸ How It Works  
The project is structured into **three main phases**, all driven by AI-powered cleaning methods:

### **1ï¸âƒ£ Data Ingestion**
- Users can:
  - **Upload a local dataset** (.csv or .xlsx)
  - **Scrape tabular data** from a website  
  - **Fetch & merge a Kaggle dataset**  
- The system extracts, processes, and standardizes data.

### **2ï¸âƒ£ Data Cleaning (AI-Driven)**
| Column Type      | Cleaning Method Used |
|-----------------|---------------------|
| **Numerical**   | **KNN Imputation** estimates missing values based on similar rows |
| **Categorical** | **Mode Imputation** fills missing values with the most common category |
| **IDs**         | **Sequential Filling** ensures IDs remain unique and follow a logical order |
| **Outliers**    | **Isolation Forest (ML model)** detects and removes anomalies |
| **Data Types**  | **OpenAI GPT model** infers and enforces correct column types |

### **3ï¸âƒ£ Data Validation & Export**
- AI **Supervisor Agent** evaluates cleaning steps and suggests **further improvements**.  
- Users receive **a detailed report on changes and transformations applied**.  
- Cleaned data is saved in `cleaned_data/` for **further analysis or model training**.  

---

## ğŸ—ï¸ Project Structure  

```plaintext
ğŸ“‚ AI-Data-Cleaning-Pipeline
â”‚â”€â”€ ğŸ“‚ data_cleaning
â”‚   â”‚â”€â”€ cleaning_agent.py      # AI-based data cleaning module
â”‚   â”‚â”€â”€ config.py              # Loads API keys & configurations
â”‚   â”‚â”€â”€ outlier_model.pkl      # Pre-trained CatBoost outlier model
â”‚
â”‚â”€â”€ ğŸ“‚ data_scraper
â”‚   â”‚â”€â”€ scraper.py             # Web scraping module (table extraction)
â”‚
â”‚â”€â”€ ğŸ“‚ user_interface
â”‚   â”‚â”€â”€ uploader.py            # Handles dataset uploads & merging
â”‚
â”‚â”€â”€ ğŸ“‚ tests
â”‚   â”‚â”€â”€ test_cleaning.py       # Unit tests for data cleaning functions
â”‚   â”‚â”€â”€ test_scraper.py        # Unit tests for web scraping functions
â”‚
â”‚â”€â”€ ğŸ“‚ cleaned_data             # Stores all cleaned datasets
â”‚â”€â”€ main.py                    # Entry point for the pipeline
â”‚â”€â”€ gpt_api_key.txt             # OpenAI API key (DO NOT COMMIT TO GITHUB)
â”‚â”€â”€ requirements.txt            # Python dependencies
â”‚â”€â”€ README.md                   # Project documentation
